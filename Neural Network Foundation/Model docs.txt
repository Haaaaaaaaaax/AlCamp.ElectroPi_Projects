
Project Documentation: Text Embedding and Regression Modeling with BERT and Neural Networks

1. Project Overview

This project aims to predict a continuous target variable (`UnitPrice`) from textual descriptions (`Description`), (`Quantity`) and (`Country`). We leverage BERT embeddings for text representation, apply PCA for dimensionality reduction, and train a neural network regressor.

2. Steps

Data Preparation
- Collected dataset with ~500,000 samples.
- Text descriptions limited to max 10 words per sentence.
- Target variable: `UnitPrice`.
- Quantity : the quantity of order
- Country

Text Embedding
- Used Hugging Face’s BERT tokenizer and model (`bert-base-uncased`).
- Tokenized texts with truncation and padding (max length 50).
- Generated embeddings by mean pooling the last hidden states.
- Embeddings generated in batches with GPU acceleration for speed.

Dimensionality Reduction
- Applied PCA to BERT embeddings to reduce dimensionality.
- Set PCA to retain 20 dimensions.

Feature Engineering
- Scaled PCA embeddings using `StandardScaler`.
- Combined scaled embeddings with other numeric features.
- Prepared final feature matrix for model input.

Model Design and Training
- Built a feedforward neural network with 2 hidden layers (128, 64 neurons).
- Used ReLU activations and dropout (0.3) for regularization.
- Output layer produces a single continuous value (regression).
- Optimizer: Adamax with learning rate 1e-4.
- Loss function: Mean Squared Error (MSELoss).
- Trained for 10 epochs with batch size 16.
- Implemented training loop with tqdm progress bars.
- Evaluated model after each epoch on validation set.

Model Evaluation
- Metrics computed on test set:
  - Root Mean Squared Error (RMSE)
  - Mean Absolute Error (MAE)
  - R² score (coefficient of determination)

Model Saving and Loading
- Saved model weights with `torch.save` as `ANN.pth`.
- Saved transformer objects (encoder, scaler, PCA) using `pickle`.
- Loaded saved objects for inference.

3. Findings

- PCA reduced embedding dimensionality from 768 to 20.
- Model achieved Test RMSE ~0.65, MAE ~0.47, and R² ~0.58.
- Validation loss steadily decreased over epochs, indicating learning.
- No accuracy metric used since task is regression.
- GPU significantly accelerated embedding generation and model training.

4. Implementation Details

Environment and Libraries
- Python 3.x
- PyTorch (specify version)
- Transformers (Hugging Face)
- scikit-learn
- numpy, pandas, tqdm
- Hardware: GPU (if available), otherwise CPU

BERT Embedding Code Snippet

def encode(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=50)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()
    return embeddings.cpu().numpy()

PCA Dimensionality Reduction

from sklearn.decomposition import PCA

pca = PCA(n_components=20)
pca_result = pca.fit_transform(embeddings_array)

Feature Scaling and Combining

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
embeddings_scaled = scaler.fit_transform(pca_result)
final_features = np.hstack([embeddings_scaled, other_numeric_features])

Neural Network Architecture

class ANNModel(nn.Module):
    def __init__(self, input_size, hidden1, hidden2, dropout_rate, output_size):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, hidden1),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden1, hidden2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden2, output_size)
        )

    def forward(self, x):
        return self.net(x)

Training Loop Highlights

- Used `Adamax` optimizer.
- Loss: `nn.MSELoss()`.
- Used tqdm for progress bars.

Evaluation Metrics Computation

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

rmse = mean_squared_error(targets, preds, squared=False)
mae = mean_absolute_error(targets, preds)
r2 = r2_score(targets, preds)

Model Saving and Loading

torch.save(model.state_dict(), 'ANN.pth')

model = ANNModel(input_size, hidden1, hidden2, dropout_rate, output_size)
model.load_state_dict(torch.load('ANN.pth'))
model.to(device)
model.eval()

5. Notes and Recommendations

- Accuracy is not applicable for regression tasks.
- Ensure consistent preprocessing pipeline between training and inference.
- Consider hyperparameter tuning and advanced architectures for further improvement.
- Save all preprocessing objects (tokenizer, scaler, PCA) to ensure reproducibility.

End of documentation.
